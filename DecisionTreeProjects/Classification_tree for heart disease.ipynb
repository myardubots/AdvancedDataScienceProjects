{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### classification tree are useful machine learning method when you need to know how the decision are being made","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importing libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for graphs\n\nfrom sklearn.tree import DecisionTreeClassifier # classifier\nfrom sklearn.tree import plot_tree # to draw classifiction tree\n\nfrom sklearn.model_selection import train_test_split # splitting data into training and test sets\nfrom sklearn.model_selection import cross_val_score # crossvalidaton\n\nfrom sklearn.metrics import confusion_matrix # create confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix # plotting confusion matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/heart-disease-uci/heart.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Identifying and handling missing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.T.apply(lambda columns: columns.nunique(), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### lucikly ther were no missing data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Splitting data into dependent and independent variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = df.target\nX = df.drop(['target'], axis=1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Format the data one hot encoding\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_encoded = pd.get_dummies(X, columns=['cp','restecg','slope','thal'])\nX_encoded.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y, random_state=42)\nclf_dt = DecisionTreeClassifier(random_state=42)\nclf_dt = clf_dt.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(clf_dt, X_test, Y_test, display_labels=[\"Does not have heart disease\",\"Have heart disease\"]);\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## pruning the tree \n##### tackle overfitting and imporving the performance of decision tree classifier ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"path = clf_dt.cost_complexity_pruning_path(X_train, Y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\nccp_alphas = ccp_alphas[:-1]\n\nclf_dts = []\nfor ccp_alpha in ccp_alphas:\n    clf_dt = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf_dt.fit(X_train, Y_train)\n    clf_dts.append(clf_dt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_scores = [clf_dt.score(X_train, Y_train) for clf_dt in clf_dts]\ntest_scores = [clf_dt.score(X_test, Y_test) for clf_dt in clf_dts]\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\", drawstyle=\"steps-post\")\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the graph above, we see that the accuracy for the **Testing Dataset** hits its maximum value when `alpha` is about **0.016**. After this value for `alpha`, the accuracy of the **Training Dataset** drops off and that suggest we should set `ccp_alpha=0.016`.\n\nHowever, since there are many ways we could have divided the original dataset into **Training** and **Testing** datasets, how do we know we used the best **Training Dataset** and how do we know we used the best **Testing Dataset**? Typically, we answer this question with **10-Fold Cross Validation**. So that's what we're going to do now, and we'll do it with the `cross_val_score()` function.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Finding the best alpha\nThe graphs we just drew suggest one value for alpha, **0.016**, but another\nset of data might suggest another optimal value. \n\n<!-- **Terminology Alert!!!** Since, ultimately, we have to decide on one value for `alpha`, and\nthe **Decision Tree** algorithm will not do this for us, `alpha` is called a **Hyperparameter** to differentiate it from the parameters that the **Decision Tree** algorithm can take care of on its own. -->\n\nFirst, let's demonstrate that different training and testing datasets result in trees with different accuracies:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_dt = DecisionTreeClassifier(random_state=42, ccp_alpha=0.03)\nscores = cross_val_score(clf_dt, X_train, Y_train, cv=5)\ndf = pd.DataFrame(data={'tree': range(5), 'accuracy': scores})\n\ndf.plot(x='tree', y='accuracy', marker='o', linestyle='--')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we see that using different **Training** and **Testing** data results in different accuracies.\n\nNow let's use **cross validation** to find the optimal value for `ccp_alpha`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha_loop_values = []\nfor ccp_alpha in ccp_alphas:\n    clf_dt = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    scores = cross_val_score(clf_dt, X_train, Y_train, cv=5)\n    alpha_loop_values.append([ccp_alpha, np.mean(scores), np.std(scores)])\n    \nalpha_results = pd.DataFrame(alpha_loop_values, \n                             columns=['alpha', 'mean_accuracy', 'std'])\n\nalpha_results.plot(x='alpha', \n                   y='mean_accuracy', \n                   yerr='std', \n                   marker='o', \n                   linestyle='--')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using cross validation, we can see that, over all, instead of setting `ccp_alpha=0.016`,  we need to set it to something closer to **0.014**. We can find the exact value with:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha_results[(alpha_results['alpha']>0.01)\n             &\n             (alpha_results['alpha']<0.015)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ideal_ccp_alpha = alpha_results[(alpha_results['alpha']>0.01)\n                                &\n                                (alpha_results['alpha']<0.015)]['alpha']\nideal_ccp_alpha","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ideal_ccp_alpha=float(ideal_ccp_alpha)\nideal_ccp_alpha","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_dt_pruned = DecisionTreeClassifier(random_state=42, \n                                       ccp_alpha=ideal_ccp_alpha)\nclf_dt_pruned = clf_dt_pruned.fit(X_train, Y_train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(clf_dt_pruned, \n                      X_test, \n                      Y_test, \n                      display_labels=[\"Does not have HD\", \"Has HD\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7.5))\nplot_tree(clf_dt_pruned, \n          filled=True, \n          rounded=True, \n          class_names=[\"No HD\", \"Yes HD\"], \n          feature_names=X.columns) ;","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
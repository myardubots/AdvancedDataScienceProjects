{"cells":[{"metadata":{},"cell_type":"markdown","source":" <h2 align=center> Machine Learning Visualization Tools </h2>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### About the Dataset:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*Concrete Compressive Strength Dataset*\n\nConcrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. \n- Number of instances 1030\n- Number of Attributes 9\n- Attribute breakdown 8 quantitative input variables, and 1 quantitative output variable ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The aim of the dataset is to predict concrete compressive strength of high performance concrete (HPC). HPC does not always means high strength but covers all kinds of concrete for special applications that are not possible with standard concretes. Therefore, our target value is:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Target y**\n- Concrete compressive strength [MPa]\n\nIn this case the compressive strength is the cylindrical compressive strength meaning a cylindrical sample (15 cm diameter; 30 cm height) was used for testing. The value is a bit smaller than testing on cubic samples. Both tests assess the uniaxial compressive strength. Usually, we get both values if we buy concrete.\n\nTo predict compressive strengths, we have these features available:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Input X**:\n- Cement $[\\frac{kg}{m^3}]$\n- Blast furnace slag $[\\frac{kg}{m^3}]$\n- Fly ask $[\\frac{kg}{m^3}]$\n- Water $[\\frac{kg}{m^3}]$\n- Plasticizer $[\\frac{kg}{m^3}]$\n- Coarse aggregate $[\\frac{kg}{m^3}]$\n- Fine aggregate $[\\frac{kg}{m^3}]$\n- Age $[d]$","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standard imports\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd\nimport warnings\nimport numpy as np\nfrom pylab import rcParams\nimport seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\nrcParams['figure.figsize'] = 15, 10\n\nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Dataset Exploration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data\n\ndf = pd.read_csv('../input/concrete.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing the Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Specify the features and target of interest\nfeatures = [\"cement\",\"slag\",\"ash\",\"water\",\"splast\",\"coarse\",\"fine\",\"age\"]\ntarget = 'strength'\n# Get the X and y data from the DataFrame\nX = df[features]\ny = df[target]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Pairwise Scatterplot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Feature Importances","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###### The feature engineering process involves selecting the minimum required features to produce a valid model because the more features a model contains, the more complex it is (and the more sparse the data), therefore the more sensitive the model is to errors due to variance. A common approach to eliminating features is to describe their relative importance to a model, then eliminate weak features or combinations of features and re-evalute to see if the model fairs better during cross-validation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#from yellowbrick.features.importances import FeatureImportances\nfrom yellowbrick.model_selection import FeatureImportances\nfrom sklearn.linear_model import Lasso\n\n# Create a new figure\nfig = plt.figure()\nax = fig.add_subplot()\n\n# Title case the feature for better display and create the visualizer\nlabels = list(map(lambda s: s.title(), features))\nviz = FeatureImportances(Lasso(), ax=ax, labels=labels, relative=False)\n\n# Fit and show the feature importances\nviz.fit(X, y)\nviz.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target Visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Frequently, machine learning problems in the real world suffer from the curse of dimensionality; you have fewer training instances than you’d like and the predictive signal is distributed (often unpredictably!) across many different features.Sometimes when the your target variable is continuously-valued, there simply aren’t enough instances to predict these values to the precision of regression. In this case, we can sometimes transform the regression problem into a classification problem by binning the continuous values into makeshift classes.To help the user select the optimal number of bins, the BalancedBinningReference visualizer takes the target variable y as input and generates a histogram with vertical lines indicating the recommended value points to ensure that the data is evenly distributed into each bin.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.target import BalancedBinningReference\n\n# Instantiate the visualizer\nvisualizer = BalancedBinningReference()\n\nvisualizer.fit(y)          # Fit the data to the visualizer\nvisualizer.poof()          # Draw/show/poof the data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluating Lasso Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### A prediction error plot shows the actual targets from the dataset against the predicted values generated by our model. This allows us to see how much variance is in the model. Data scientists can diagnose regression models using this plot by comparing against the 45 degree line, where the prediction exactly matches the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.regressor import PredictionError\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = PredictionError(Lasso(), size=(800, 600))\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\n\n# Call finalize to draw the final yellowbrick-specific elements\nvisualizer.finalize()\n\n# Get access to the axes object and modify labels\nvisualizer.ax.set_xlabel(\"measured concrete strength\")\nvisualizer.ax.set_ylabel(\"predicted concrete strength\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization of Test-set Errors\n\n###### Using YellowBrick we can show the residuals (difference between the predicted value and the truth) both for the training set and the testing set (respectively blue and green).\n###### A common use of the residuals plot is to analyze the variance of the error of the regressor. If the points are randomly dispersed around the horizontal axis, a linear regression model is usually appropriate for the data; otherwise, a non-linear model is more appropriate.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.regressor import ResidualsPlot\n\nvisualizer = ResidualsPlot(Lasso(), size=(800,600))\n\nvisualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\nvisualizer.score(X_test, y_test)  # Evaluate the model on the test data\ng = visualizer.poof()             # Draw/show/poof the data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 9: Cross Validation Scores","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Generally we determine whether a given model is optimal by looking at it’s F1, precision, recall, and accuracy (for classification), or it’s coefficient of determination (R2) and error (for regression). However, real world data is often distributed somewhat unevenly, meaning that the fitted model is likely to perform better on some sections of the data than on others. Yellowbrick’s CVScores visualizer enables us to visually explore these variations in performance using different cross validation strategies.\n##### Cross-validation starts by shuffling the data (to prevent any unintentional ordering errors) and splitting it into k folds. Then k models are fit on k−1k of the data (called the training split) and evaluated on 1k of the data (called the test split). The results from each evaluation are averaged together for a final score, then the final model is fit on the entire dataset for operationalization.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom yellowbrick.model_selection import CVScores\n\n# Create a new figure and axes\n_, ax = plt.subplots()\n\ncv = KFold(12)\n\noz = CVScores(\n    Lasso(), ax=ax, cv=cv, scoring='r2', size=(800,500)\n)\n\noz.fit(X_train, y_train)\noz.poof();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Learning Curves","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### A learning curve shows the relationship of the training score versus the cross validated test score for an estimator with a varying number of training samples. This visualization is typically used to show two things:\n1. How much the estimator benefits from more data (e.g. do we have “enough data” or will the estimator get better if used in an online fashion).\n2. If the estimator is more sensitive to error due to variance vs. error due to bias.\n\n##### If the training and cross-validation scores converge together as more data is added (shown in the left figure), then the model will probably not benefit from more data. If the training score is much greater than the validation score then the model probably requires more training examples in order to generalize more effectively.\n\n##### The curves are plotted with the mean scores, however variability during cross-validation is shown with the shaded areas that represent a standard deviation above and below the mean for all cross-validations. If the model suffers from error due to bias, then there will likely be more variability around the training score curve. If the model suffers from error due to variance, then there will be more variability around the cross validated score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.model_selection import LearningCurve\nfrom sklearn.linear_model import LassoCV\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 15, 10\n\n# Create the learning curve visualizer\nsizes = np.linspace(0.3, 1.0, 10)\n\n# Create the learning curve visualizer, fit and poof\nviz = LearningCurve(LassoCV(), train_sizes=sizes, scoring='r2')\nviz.fit(X, y)\nviz.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Hyperparamter Tuning\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The `AlphaSelection` Visualizer demonstrates how different values of alpha influence model selection during the regularization of linear models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.regressor import AlphaSelection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alphas = np.logspace(-10,1,400)\n\nmodel = LassoCV(alphas=alphas)\nvisualizer = AlphaSelection(model, size=(800,600))\n\nvisualizer.fit(X,y)\nvisualizer.poof();","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}